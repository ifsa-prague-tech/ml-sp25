{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "634fc08b-4970-41c4-b99b-6fddaaa9ecd2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Classic NLP - Labs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "073bab52-fe2e-4d8e-86cf-fff52a9c120c",
   "metadata": {},
   "source": [
    "This exercise aims to categorize all texts into distinct groups that share similar themes or subjects. The grouping is determined by analyzing the frequency of certain words used in the reports, by applying the Latent Dirichlet Allocation (LDA) algorithm.\n",
    "\n",
    "#### Dataset\n",
    "@dataset {consolidated_climate_dataset,  \n",
    "    author = {Joseph Pollack},  \n",
    "    title = {Climate Guard Toxic Agent - Dataset},  \n",
    "    year = {2024},  \n",
    "    publisher = {Hugging Face},  \n",
    "    url = { https://huggingface.co/datasets/Tonic/Climate-Guard-Toxic-Agent }  \n",
    "}\n",
    "\n",
    "<br>\n",
    "\n",
    "Documentation:\n",
    "* **Latent Dirichlet Allocation** (topic modelling) in [Gensim library](https://radimrehurek.com/gensim/models/ldamodel.html)\n",
    "* **LDA visualization** in [pyLDAvis library](https://github.com/bmabey/pyLDAvis)\n",
    "* **Lemmatizer** in [NLTK library](https://www.nltk.org/api/nltk.stem.WordNetLemmatizer.html?highlight=wordnet)\n",
    "* **Stopwords** in [NLTK library](https://www.nltk.org/howto/corpus.html)\n",
    "\n",
    "<br>\n",
    "@Ricardo Almeida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f044c6f-0045-4e4e-aca1-4d71f846ed1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk pyLDAvis wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b44d909-eeb3-4d52-b83c-ae2458891ce4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, matutils, models\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyLDAvis\n",
    "from pyLDAvis.gensim_models import prepare\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f2b353-6b8e-4c9d-b1da-cba072ed6c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = nltk.download('stopwords', quiet=True)\n",
    "_ = nltk.download('wordnet', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a990ae40-6113-4426-ab89-10d83cb2a72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_column = 'text'\n",
    "\n",
    "num_passes=50\n",
    "num_iterations=100\n",
    "\n",
    "word_min_doc_occurences=0.05 # decimal as percentage of docs, or integer as absolute number of doc occurences\n",
    "num_topics = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4f2c2e-4f96-4087-a119-815d02bb30a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfc93b1-4573-4393-94a3-49777622b4d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "Text segments dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95839710-cdf2-4ad5-ae93-c4a9197a588b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"data/climate_text_sample.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50986a0b-7dc8-481d-8294-ddbd1e2349b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26fb6c7-814d-4ec8-b090-8568a2d99747",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in df[0:3].text:\n",
    "    print(f\"{t} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6461dfc1-3de0-4a2c-96f9-c29049b15201",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3007660e-bf4e-4321-814c-07a04b33247f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Pre-processing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31ab4cf-09ba-4ae0-b19b-db05e7de687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a0fb66-baf1-4460-93ce-91c51935e34e",
   "metadata": {},
   "source": [
    "### Task #1\n",
    "\n",
    "*Task*: Perform the text preprocessing\n",
    "\n",
    "Consider applying these transformations (not necessarily in this order!):\n",
    "\n",
    "- Remove digits: removing tokens that are purely numeric or start with a number\n",
    "- Remove punctuation\n",
    "- Remove stopwords: removing  words that do not add significant meaning *Suggestion*: you may use `stopwords.words('english')` set from NLTK library\n",
    "- Blacklist removal: removing specific words\n",
    "- Tokenization: splits the text into individual words (tokens). *Suggestion*: you may use `RegexpTokenizer(r'\\w+')` which already removes punctuation\n",
    "- Lemmatization: reduces words to their base, dictionary form.  *Suggestion*: you may use `WordNetLemmatizer()`\n",
    "- Lowercasing: converting all text to lowercase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f96d10-6af6-4c72-ad7d-2cee022cf081",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[text_column] = df[text_column].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732606d5-bd2c-4353-9271-54120a6f9949",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_blacklist = ['climate', 'change']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9e590c-bc50-49c5-b760-910aed4174b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process text\n",
    "def process_text(text):\n",
    "\n",
    "    # Tokenize the text (split text into words, remove punctuation)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    lemmatizer = WordNetLemmatizer()    \n",
    "    stop_words_set = set(stopwords.words('english'))\n",
    "    processed_tokens = [\n",
    "        lemmatizer.lemmatize(token)  # Reduce words to their base or dictionary form\n",
    "        for token in tokens\n",
    "        if token not in stop_words_set and not token.isnumeric()\n",
    "    ]\n",
    "    # remove blacklisted words\n",
    "    processed_tokens = [token for token in processed_tokens if token not in word_blacklist]\n",
    "    # remove tokens starting with a number\n",
    "    processed_tokens = [token for token in processed_tokens if not token.isnumeric()]\n",
    "    \n",
    "    return processed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f5d1c8-1516-4d78-81b4-396fa6597a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply text preprocessing\n",
    "\n",
    "df['tokens'] = df[text_column].apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec12ff67-f73c-4767-9403-93b77348c952",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaaaf58-ceb6-48e6-be96-546bfa65033d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a document-term matrix. Use CountVectorizer to convert document into a matrix of token counts\n",
    "vectorizer = CountVectorizer(analyzer=lambda x: x, min_df=word_min_doc_occurences)\n",
    "matrix = vectorizer.fit_transform(df['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67cd8b6-270b-47c7-9c4e-e7ab694b5e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(matrix[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77469c06-9d23-45a4-95c1-2b956e6d594f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a gensim-friendly iterable\n",
    "corpus = matutils.Sparse2Corpus(matrix, documents_columns=False)\n",
    "\n",
    "# Construct a document-term vocabulary (dictionary)\n",
    "id2word = dict((v, k) for k, v in vectorizer.vocabulary_.items())\n",
    "\n",
    "# Create a gensim Dictionary\n",
    "id2word_dict = corpora.Dictionary(df['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68ad5b7-8b5f-430f-8f22-02f6696823e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c45820d-d1e7-4c1f-a3ab-223a8946860f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = models.LdaModel(corpus=corpus, num_topics=num_topics, id2word=id2word, passes=num_passes, iterations=num_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cabe90-73e3-4310-ab04-f07ecb1f5667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dict_2_gensim_dict(id_to_word_dict):    \n",
    "    # Invert the mapping (now keys are tokens and values are the token IDs)\n",
    "    token_to_id = {v: k for k, v in id_to_word_dict.items()}\n",
    "    \n",
    "    # Create a gensim Dictionary from the token to ID mapping\n",
    "    gensim_dict = corpora.dictionary.Dictionary()\n",
    "    gensim_dict.token2id = token_to_id\n",
    "    \n",
    "    # Optional\n",
    "    #gensim_dict.num_docs = num_documents  # Set the number of documents, you need to define num_documents\n",
    "    #gensim_dict.dfs = {token_to_id[token]: 1 for token in token_to_id}  # Assuming each token appears in 1 document\n",
    "\n",
    "    return gensim_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5f042b-1562-488b-9bde-0f444cd008a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "prepare(topic_model=lda, corpus=corpus, dictionary=convert_dict_2_gensim_dict(id2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711972b6-1afb-41d7-933d-7a0543ebdaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract topics and their word distributions\n",
    "topics = lda.print_topics(num_words=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e557b4d-5088-4be2-8e47-31216c712a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topics = lda.top_topics(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7cdc02-cec3-4dd2-b5b5-f2754bc175da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, _ in enumerate(top_topics):\n",
    "    print(f\"{[w[1] for w in top_topics[i][0]]} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acb275f-75cb-45bf-b7c5-ecba525890fd",
   "metadata": {},
   "source": [
    "### Task #2\n",
    "\n",
    "*Task*: Visualize topics with wordclouds.\n",
    "\n",
    "Using `WordCloud` from [wordcloud library](https://github.com/amueller/word_cloud), create a word cloud for each topic, based on the already prepared word dicts.\n",
    "\n",
    "Then show the wordcloud with `plt.imshow(your_wordcloud)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852f96c3-fbdf-41d2-8b8b-da05188cc552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wordcloud(t_id, words_dict):\n",
    "    # Create a WordCloud object\n",
    "    wc = WordCloud(background_color='white')\n",
    "    wc.generate_from_frequencies(words_dict)\n",
    "\n",
    "    # Display the generated word cloud\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'\\n\\nTopic {t_id+1}\\n')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadba356-8a5d-4cdc-95da-33cf9494c289",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the topics using word clouds, one for each topic\n",
    "\n",
    "for topic_id, topic in enumerate(topics):\n",
    "    # Process each formatted topic string\n",
    "    # Extracts word and weight (probability), converts weight to float and collects in dict\n",
    "    words_dict = {word: float(weight) for weight, word in \n",
    "                  [tuple(word_prob.split('*')) for word_prob in topic[1].split(' + ')]}\n",
    "    words_dict = {key.strip('\"'): value for key, value in words_dict.items()}\n",
    "\n",
    "    #print(words_dict)\n",
    "    #print()\n",
    "\n",
    "    create_wordcloud(topic_id, words_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b7176d-1d58-45ec-a076-0255e772bd5c",
   "metadata": {},
   "source": [
    "### Task #3\n",
    "\n",
    "*Task*: Discuss the meaningfulness of the results\n",
    "\n",
    "* Do the resulting topics make sense? Are they useful?\n",
    "* Does the (bag-of-words) approach result in a good understanding/split of the statements?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36b3799-a6c1-4b63-94dd-1c4c175a8ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
